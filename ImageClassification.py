# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18cnhlJjzasEXel_eCA1qWomP10Jf_1tF
"""

# Gamze SELVİLİ - 190102010
# Aslıhan HASTAOĞLU - 190102009
# import the necessary packages
from keras.models import Sequential
from keras.layers.core import Dense

# Option 1: define the 3072-1024-512-3 architecture using Keras
# simple feedforward neural network using the standard Keras package
model = Sequential()
model.add(Dense(1024, input_shape=(3072,), activation="sigmoid"))
model.add(Dense(512, activation="sigmoid"))
model.add(Dense(10, activation="softmax"))

# # Option 2: define the 3072-1024-512-3 architecture using tf.keras, end result is the same as Option 1.
# # implement the same network using the  tf.keras  submodule which is part of TensorFlow:
# import tensorflow as tf
# model = tf.keras.models.Sequential()
# model.add(tf.keras.layers.Dense(1024, input_shape=(3072,), activation="sigmoid"))
# model.add(tf.keras.layers.Dense(512, activation="sigmoid"))
# model.add(tf.keras.layers.Dense(10, activation="softmax"))

# The first step in training our network is to implement the network architecture itself in Keras

# import the necessary packages
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Dropout
from keras.layers.core import Dense
from keras.layers import Flatten
from keras.layers import Input
from keras.models import Model

class KerasCNN:
    @staticmethod
    def build(width, height, depth, classes):
        # initialize the input shape and channel dimension, assuming
        # TensorFlow/channels-last ordering
        inputShape = (height, width, depth)
        chanDim = -1

        # define the model input
        inputs = Input(shape=inputShape)

# Let’s start defining the body of the Convolutional Neural Network:
        # first (CONV => RELU) * 2 => POOL layer set
        x = Conv2D(32, (3, 3), padding="same")(inputs)
        x = Activation("relu")(x)
        x = BatchNormalization(axis=chanDim)(x)
        x = Conv2D(32, (3, 3), padding="same")(x)
        x = Activation("relu")(x)
        x = BatchNormalization(axis=chanDim)(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        x = Dropout(0.5)(x)

# Let’s add the fully-connected (FC) layers to the network:
        # second (CONV => RELU) * 2 => POOL layer set
        x = Conv2D(64, (3, 3), padding="same")(x)
        x = Activation("relu")(x)
        x = BatchNormalization(axis=chanDim)(x)
        x = Conv2D(64, (3, 3), padding="same")(x)
        x = Activation("relu")(x)
        x = BatchNormalization(axis=chanDim)(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        x = Dropout(0.5)(x)

        # first (and only) set of FC => RELU layers
        x = Flatten()(x)
        x = Dense(512)(x)
        x = Activation("relu")(x)
        x = BatchNormalization()(x)
        x = Dropout(0.25)(x)

        # softmax classifier
        x = Dense(classes)(x)
        x = Activation("softmax")(x)

        # create the model
        model = Model(inputs, x, name="minivggnet_keras")

        # return the constructed network architecture
        return model

# Our FC and Softmax classifier are appended onto the network. 
# We then define the neural network model and return it to the calling function.

# import the necessary packages
# from pyimagesearch.minivggnetkeras import MiniVGGNetKeras
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import classification_report
from keras.optimizers import Adam
from keras.datasets import cifar10
import matplotlib.pyplot as plt
import numpy as np
import time   # time1 = time.time(); print('Time taken: {:.1f} seconds'.format(time.time() - time1))

# load the training and testing data, then scale it into the range [0, 1]

print("[INFO] loading CIFAR-10 data...")
split = cifar10.load_data()
((trainX, trainY), (testX, testY)) = split
trainX = trainX.astype("float") / 255.0
testX = testX.astype("float") / 255.0

# convert the labels from integers to vectors
lb = LabelBinarizer()
trainY = lb.fit_transform(trainY)
testY = lb.transform(testY)

# initialize the label names for the CIFAR-10 dataset
labelNames = ["airplane", "car", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]

# Next, let’s train the model
# initialize the initial learning rate, total number of epochs to train for, and batch size
INIT_LR = 0.01
EPOCHS =30 #epochs ideal
BS = 32

# initialize the optimizer and model
print("[INFO] compiling model...")
opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)
model = KerasCNN.build(width=32, height=32, depth=3, classes=len(labelNames))
model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# train the network
print("[INFO] training network for {} epochs...".format(EPOCHS))
time1 = time.time()  
H = model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=BS, epochs=EPOCHS, verbose=1)
print('Time taken: {:.1f} seconds'.format(time.time() - time1))  

# evaluate the network
print("[INFO] evaluating network...")
predictions = model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

# plot the training/validation loss
plt.style.use("ggplot")
plt.figure(figsize = [8,6])
plt.plot(np.arange(0, EPOCHS), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, EPOCHS), H.history["val_loss"], label="val_loss")
plt.title("Loss on Training/Validation Dataset")
plt.xlabel("Epoch #", weight="bold")
plt.ylabel("Loss", weight="bold")
plt.legend(loc="right")

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.style.use("ggplot")
plt.figure(figsize = [8,6])
plt.plot(np.arange(0, EPOCHS), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, EPOCHS), H.history["val_accuracy"], label="val_acc")
plt.title("Accuracy on Training/Validation Dataset")
plt.xlabel("Epoch #", weight="bold")
plt.ylabel("Accuracy", weight="bold")
plt.legend(loc="right")

from keras.models import load_model
import cv2
from google.colab.patches import cv2_imshow

model.save("CIFAR-10_keras_model.h5")
print("Saved model to disk")
# load model
model = load_model('CIFAR-10_keras_model.h5')
image = cv2.imread("bird.jpg")
cv2_imshow(image)
image = cv2.resize(image, (32, 32))   # height, width


# scale the pixel values to [0, 1]
image = image.astype("float") / 255.0

# when working with a CNN: don't flatten the image, simply add the batch dimension (expected input to have 4 dimensions)
image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))

# make a prediction on the image
preds = model.predict(image)

# plot the prediction probability for each category
plt.figure(figsize = [10,5])   # [width, height]

x = ["airplane", "car", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]
y = [ preds[0][0], preds[0][1], preds[0][2], preds[0][3], preds[0][4], preds[0][5], preds[0][6], preds[0][7], preds[0][8], preds[0][9] ]
plt.barh(x, y, color='orange')

ticks_x = np.linspace(0, 1, 11)   # (start, end, number of ticks)
plt.xticks(ticks_x, fontsize=10, family='fantasy', color='black')
plt.yticks( size=15, color='navy' )
for i, v in enumerate(y):
    plt.text(v, i, "  "+str((v*100).round(1))+"%", color='blue', va='center', fontweight='bold')

plt.title('Prediction Probability', family='serif', fontsize=15, style='italic', weight='bold', color='olive', loc='center', rotation=0)
plt.xlabel('Probability', fontsize=12, weight='bold', color='blue')
plt.ylabel('Category', fontsize=12, weight='bold', color='navy')